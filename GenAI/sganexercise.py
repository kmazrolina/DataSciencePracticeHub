# -*- coding: utf-8 -*-
"""SGANexercise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xQ7ynZdQeyi4LyTI302BD6im-Q8Sw18z

# Loading Dataset
"""

from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.utils.data import Subset

# Define transformations for the dataset
transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Resize images to a fixed size (optional)
    transforms.ToTensor(),          # Convert images to PyTorch tensors
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize (optional)
])

# Load the full CIFAR-10 dataset
cifar10 = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)

# Find the indices of the "frog" class (class index 6)
frog_indices = [i for i, label in enumerate(cifar10.targets) if label == 6]

# Create a subset containing only frogs
frog_data = Subset(cifar10, frog_indices)

# Create DataLoader for training and testing sets
batch_size = 64
data_loader = DataLoader(frog_data, batch_size=batch_size, shuffle=True, drop_last=True)

# Ensure the data is in float16 after the batch is loaded
# Check the size of a batch
images, labels = next(iter(data_loader))

# Convert images to float16
images = images.half()  # Convert images to float16
print(f"Image batch dimensions: {images.shape}")
print(f"Label batch dimensions: {labels.shape}")

len(frog_data)

import matplotlib.pyplot as plt
import numpy as np
import torchvision.transforms as transforms

# Retrieve one batch of images and labels from the DataLoader
data_iter = iter(data_loader)
images, labels = next(data_iter)

image = images[0]  # Shape [C, H, W]


# Convert the tensor to a NumPy array for visualization
image_np = image.permute(1, 2, 0).cpu().numpy()

# Plot the image
plt.imshow(image_np)
plt.axis('off')
plt.show()

"""# SGAN

Define Generators and Discriminators
"""

import torch
import torch.nn as nn

class Generator(nn.Module):
    def __init__(self, z_dim=100, generator_feat_dim=256):
        super(Generator, self).__init__()

        # Target image dimensions: 128x128
        self.s_h, self.s_w = 128, 128
        self.s_h2, self.s_w2 = self.s_h // 2, self.s_w // 2
        self.s_h4, self.s_w4 = self.s_h2 // 2, self.s_w2 // 2
        self.s_h8, self.s_w8 = self.s_h4 // 2, self.s_w4 // 2
        self.s_h16, self.s_w16 = self.s_h8 // 2, self.s_w8 // 2  # Reduced size for final target

        # Layers
        self.dense1 = nn.Linear(z_dim, generator_feat_dim * 8 * self.s_h16 * self.s_w16)

        self.dense1_out = nn.Sequential(
            nn.BatchNorm2d(generator_feat_dim * 8),
            nn.ReLU(inplace=True)
        )

        self.deconv1 = nn.Sequential(
            nn.ConvTranspose2d(generator_feat_dim * 8, generator_feat_dim * 4, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(generator_feat_dim * 4),
            nn.ReLU(inplace=True)
        )

        self.deconv2 = nn.Sequential(
            nn.ConvTranspose2d(generator_feat_dim * 4, generator_feat_dim * 2, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(generator_feat_dim * 2),
            nn.ReLU(inplace=True)
        )

        self.deconv3 = nn.Sequential(
            nn.ConvTranspose2d(generator_feat_dim * 2, generator_feat_dim, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(generator_feat_dim),
            nn.ReLU(inplace=True)
        )

        self.deconv4 = nn.Sequential(
            nn.ConvTranspose2d(generator_feat_dim, 3, kernel_size=4, stride=2, padding=1)  # Directly to 128x128
        )

        self.out = nn.Tanh()

    def forward(self, z):
        # Project and reshape z
        fc1 = self.dense1(z)
        fc1 = fc1.view(-1, 256 * 8, self.s_h16, self.s_w16)  # Shape: (batch_size, feat_dim*8, h/16, w/16)
        fc1 = self.dense1_out(fc1)

        # Deconvolutions
        deconv1 = self.deconv1(fc1)  # Shape: (batch_size, feat_dim*4, h/8, w/8)
        deconv2 = self.deconv2(deconv1)  # Shape: (batch_size, feat_dim*2, h/4, w/4)
        deconv3 = self.deconv3(deconv2)  # Shape: (batch_size, feat_dim, h/2, w/2)
        deconv4 = self.deconv4(deconv3)  # Shape: (batch_size, 3, h, w)

        # Output image
        out = self.out(deconv4)
        return out




class Discriminator(nn.Module):
    def __init__(self, discriminator_feat_dim=64):
        super(Discriminator, self).__init__()

        # Layers
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, discriminator_feat_dim, kernel_size=4, stride=2, padding=1),  # (128x128x3 -> 64x64x64)
            nn.LeakyReLU(0.2, inplace=True)
        )

        self.conv2 = nn.Sequential(
            nn.Conv2d(discriminator_feat_dim, discriminator_feat_dim * 2, kernel_size=4, stride=2, padding=1),  # (64x64x64 -> 32x32x128)
            nn.BatchNorm2d(discriminator_feat_dim * 2),
            nn.LeakyReLU(0.2, inplace=True)
        )

        self.conv3 = nn.Sequential(
            nn.Conv2d(discriminator_feat_dim * 2, discriminator_feat_dim * 4, kernel_size=4, stride=2, padding=1),  # (32x32x128 -> 16x16x256)
            nn.BatchNorm2d(discriminator_feat_dim * 4),
            nn.LeakyReLU(0.2, inplace=True)
        )

        self.conv4 = nn.Sequential(
            nn.Conv2d(discriminator_feat_dim * 4, discriminator_feat_dim * 8, kernel_size=4, stride=2, padding=1),  # (16x16x256 -> 8x8x512)
            nn.BatchNorm2d(discriminator_feat_dim * 8),
            nn.LeakyReLU(0.2, inplace=True)
        )

        self.conv5 = nn.Sequential(
            nn.Conv2d(discriminator_feat_dim * 8, discriminator_feat_dim * 16, kernel_size=4, stride=2, padding=1),  # (8x8x512 -> 4x4x1024)
            nn.BatchNorm2d(discriminator_feat_dim * 16),
            nn.LeakyReLU(0.2, inplace=True)
        )

        self.fc = nn.Sequential(
            nn.Linear(discriminator_feat_dim * 16 * 4 * 4, 1),  # Flattened input from (4x4x1024)
            nn.Sigmoid()
        )

    def forward(self, x):
        # Forward pass through the convolutional layers
        h0 = self.conv1(x)  # Shape: (batch_size, 64, 64, 64)
        h1 = self.conv2(h0)  # Shape: (batch_size, 128, 32, 32)
        h2 = self.conv3(h1)  # Shape: (batch_size, 256, 16, 16)
        h3 = self.conv4(h2)  # Shape: (batch_size, 512, 8, 8)
        h4 = self.conv5(h3)  # Shape: (batch_size, 1024, 4, 4)

        # Flatten and pass through fully connected layer
        h4 = h4.view(h4.shape[0], -1)  # Flatten to (batch_size, 1024*4*4)
        out = self.fc(h4)  # Output shape: (batch_size, 1)

        return out

# Define device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

"""Training Loop"""

import matplotlib.pyplot as plt
import torchvision.utils as vutils
import copy
from torch import optim

# Initialize models
def init_models(N):
    G = [Generator().to(device) for _ in range(N)]
    D = [Discriminator().to(device) for _ in range(N)]
    return G, D

# Define dimensions and number of models
latent_dim = 100      # Latent space dimension for the generator input
N = 5                 # Number of local generator-discriminator pairs
I = 100               # Number of training iterations
ID = 1                # Number of discriminator updates per generator update

# Initialize models
G, D = init_models(N)
G0 = Generator().to(device)
D0 = Discriminator().to(device)

# Optimizers for local and global models
optimizers_G = [optim.Adam(g.parameters(), lr=0.0002, betas=(0.5, 0.999)) for g in G]
optimizers_D = [optim.Adam(d.parameters(), lr=0.0002, betas=(0.5, 0.999)) for d in D]
optimizer_G0 = optim.Adam(G0.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D0 = optim.Adam(D0.parameters(), lr=0.0002, betas=(0.5, 0.999))

# Lists to store losses
losses_G0 = []
losses_D0 = []
losses_Gn = []
losses_Dn = []

# Training Loop
for epoch in range(I):  # Total number of epochs
    for real_data, _ in data_loader:  # Load real images
        real_data = real_data.to(device)
        batch_size = real_data.size(0)
        noise = torch.randn(batch_size, latent_dim, device=device)

        # Step 1: Train Local (Gn, Dn) Pairs
        for n in range(N):
            for _ in range(ID):  # Update Dn ID times
                D[n].zero_grad()
                fake_data = G[n](noise).detach()  # Generate fake data
                real_loss = nn.BCELoss()(D[n](real_data), torch.ones(batch_size, 1, device=device))
                fake_loss = nn.BCELoss()(D[n](fake_data), torch.zeros(batch_size, 1, device=device))
                loss_D = real_loss + fake_loss
                loss_D.backward()
                optimizers_D[n].step()

            # Update Gn
            G[n].zero_grad()
            fake_data = G[n](noise)
            loss_G = nn.BCELoss()(D[n](fake_data), torch.ones(batch_size, 1, device=device))
            loss_G.backward()
            optimizers_G[n].step()

            # Store local losses
            losses_Dn.append(loss_D.item())
            losses_Gn.append(loss_G.item())

        # Step 2: Create Messenger Discriminators D_msg
        D_msg = [copy.deepcopy(D[n]) for n in range(N)]

        # Step 3: Train G0 against Messenger Discriminators
        G0.zero_grad()
        loss_G0 = 0
        for D_msg_n in D_msg:
            fake_data = G0(noise)
            loss_G0 += nn.BCELoss()(D_msg_n(fake_data), torch.ones(batch_size, 1, device=device))
        loss_G0.backward()
        optimizer_G0.step()
        losses_G0.append(loss_G0.item())

        # Step 4: Train Messenger Discriminators (D_msg) against G0
        for D_msg_n in D_msg:
            D_msg_n.zero_grad()
            fake_data = G0(noise).detach()
            real_loss_msg = nn.BCELoss()(D_msg_n(real_data), torch.ones(batch_size, 1, device=device))
            fake_loss_msg = nn.BCELoss()(D_msg_n(fake_data), torch.zeros(batch_size, 1, device=device))
            loss_D_msg = real_loss_msg + fake_loss_msg
            loss_D_msg.backward()

        # Step 5: Train D0 against all local generators
        D0.zero_grad()
        loss_D0 = 0
        for n in range(N):
            fake_data = G[n](noise).detach()
            loss_D0 += nn.BCELoss()(D0(fake_data), torch.zeros(batch_size, 1, device=device))
        loss_D0 += nn.BCELoss()(D0(real_data), torch.ones(batch_size, 1, device=device))
        loss_D0.backward()
        optimizer_D0.step()
        losses_D0.append(loss_D0.item())

    print(f"Epoch {epoch}, G0 Loss: {np.mean(losses_G0)}")
    # Save and display an image from G0
    with torch.no_grad():
        sample_noise = torch.randn(16, latent_dim, device=device)  # Generate noise
        generated_images = G0(sample_noise).view(-1, 3, 128, 128)  # Generate images
        grid = vutils.make_grid(generated_images, nrow=4, normalize=True)

        # Save image to disk
        image_filename = f"epoch_{epoch}_G0.png"
        vutils.save_image(grid, image_filename)

        # Display image
        plt.figure(figsize=(8, 8))
        plt.axis("off")
        plt.title(f"G0 Images at Epoch {epoch}")
        plt.imshow(grid.permute(1, 2, 0).cpu())
        plt.show()

# Plot Loss Curves
plt.figure(figsize=(12, 6))
plt.title("Loss During Training")
plt.plot(losses_D0, label="D0 Loss")
plt.plot(losses_G0, label="G0 Loss")
plt.plot(losses_Dn, label="Dn Loss (Local)")
plt.plot(losses_Gn, label="Gn Loss (Local)")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()