# -*- coding: utf-8 -*-
"""SGAN_frogs_generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y1-8EIDm_SSraRZrC6Lou7qiMbIbLuTy

# SGAN: An Alternative Training of Generative Adversarial Networks

Interpretation for frog images generation.
"""

from google.colab import drive
drive.mount('/content/drive')
path_to_gdrive = "/content/drive/MyDrive/frog_generator"

"""## Download the dataset

Contains frog images on white backgroud üê∏
image size 224x224
"""

!git clone -n --depth=1 --filter=tree:0 https://github.com/jonshamir/frog-dataset && \
cd frog-dataset && \
git sparse-checkout set --no-cone data-224 && \
git checkout

"""## Dataset loader"""

import torch
import torch.nn as nn
from torchvision import  transforms
import numpy as np
import matplotlib.pyplot as plt
import torchvision.utils as vutils
import copy
from torch import optim
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os

class CustomImageDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        """
        Args:
            data_dir (str): Path to the directory containing the .png images.
            transform (callable, optional): Optional transform to be applied on a sample.
        """
        self.data_dir = data_dir
        self.transform = transform

        # Get all .png files in the directory
        self.image_files = [f for f in os.listdir(data_dir) if f.endswith('.png')]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = os.path.join(self.data_dir, self.image_files[idx])
        image = Image.open(img_name).convert('RGB')  # Open and convert to RGB

        if self.transform:
            image = self.transform(image)

        # Return image and a dummy label (assuming there is no label in the dataset)
        label = 0  # Placeholder label, replace if you have actual labels
        return image, label



# Define transformations
img_size = 224  # Set the image size for resizing
transform = transforms.Compose([
    transforms.Resize((img_size, img_size)),  # Resize images to 64x64
    transforms.ToTensor(),                    # Convert images to PyTorch tensors
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize images
])

# Path to the dataset directory
dataset_dir = "frog-dataset/data-224"

# Create the dataset and DataLoader
dataset = CustomImageDataset(data_dir=dataset_dir, transform=transform)
data_loader = DataLoader(dataset, batch_size=16, shuffle=True, drop_last=True)

# Check if the data loads properly
images, labels = next(iter(data_loader))

# Convert images to float16 if needed
images = images.half()  # Convert images to float16
print(f"Num of images: {len(dataset)}")
print(f"Image batch dimensions: {images.shape}")

"""Example Image"""

# Retrieve one batch of images and labels from the DataLoader
data_iter = iter(data_loader)
images, labels = next(data_iter)

image = images[0]  # Shape [C, H, W]


# Convert the tensor to a NumPy array for visualization
image_np = image.permute(1, 2, 0).cpu().numpy()

# Plot the image
plt.imshow(image_np)
plt.axis('off')
plt.show()

"""Bacause the frogs are always on white background, lets prepare function that generates noise that is sparse (most of the image should be white anyways)

## Define Generator and Discriminator classes
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class Generator(nn.Module):
    def __init__(self, z_dim=100, generator_feat_dim=256, img_size=224):
        super(Generator, self).__init__()

        # Pre-calculating size after each deconvolution step
        self.s_h16, self.s_w16 = img_size // 16, img_size // 16

        # Fully connected layer
        self.dense1 = nn.Linear(z_dim, generator_feat_dim * 8 * self.s_h16 * self.s_w16)
        self.dense1_out = nn.Sequential(
            nn.BatchNorm2d(generator_feat_dim * 8),
            nn.ReLU(inplace=True)
        )

        # Deconvolutional layers
        self.deconv1 = nn.Sequential(
            nn.ConvTranspose2d(generator_feat_dim * 8, generator_feat_dim * 4, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(generator_feat_dim * 4),
            nn.ReLU(inplace=True)
        )
        self.deconv2 = nn.Sequential(
            nn.ConvTranspose2d(generator_feat_dim * 4, generator_feat_dim * 2, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(generator_feat_dim * 2),
            nn.ReLU(inplace=True)
        )
        self.deconv3 = nn.Sequential(
            nn.ConvTranspose2d(generator_feat_dim * 2, generator_feat_dim, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(generator_feat_dim),
            nn.ReLU(inplace=True)
        )
        self.deconv4 = nn.Sequential(
            nn.ConvTranspose2d(generator_feat_dim, 3, kernel_size=4, stride=2, padding=1)
        )
        self.batchnorm = nn.BatchNorm2d(3)
        self.out = nn.Tanh()

    def forward(self, z):

        # Project and reshape
        fc1 = self.dense1(z)
        fc1 = fc1.view(-1, 256 * 8, self.s_h16, self.s_w16)
        fc1 = self.dense1_out(fc1)

        # Deconvolutional layers
        x = self.deconv1(fc1)
        x = self.deconv2(x)
        x = self.deconv3(x)
        x = self.deconv4(x)

        x = self.batchnorm(x)

        # Output image
        return self.out(x)


class Discriminator(nn.Module):
    def __init__(self, discriminator_feat_dim=224, img_size=224):
        super(Discriminator, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, discriminator_feat_dim, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(discriminator_feat_dim, discriminator_feat_dim * 2, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(discriminator_feat_dim * 2),
            nn.LeakyReLU(0.2, inplace=True)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(discriminator_feat_dim * 2, discriminator_feat_dim * 4, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(discriminator_feat_dim * 4),
            nn.LeakyReLU(0.2, inplace=True)
        )
        self.conv4 = nn.Sequential(
            nn.Conv2d(discriminator_feat_dim * 4, discriminator_feat_dim * 8, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(discriminator_feat_dim * 8),
            nn.LeakyReLU(0.2, inplace=True)
        )
        self.conv5 = nn.Sequential(
            nn.Conv2d(discriminator_feat_dim * 8, discriminator_feat_dim * 16, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(discriminator_feat_dim * 16),
            nn.LeakyReLU(0.2, inplace=True)
        )

        # Fully connected layer
        self.fc = nn.Sequential(
            nn.Linear(discriminator_feat_dim * 16 * (img_size // 32) ** 2, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)

        # Flatten and pass through the fully connected layer
        x = x.view(x.shape[0], -1)
        return self.fc(x)

"""## Model Init"""

# Define device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device


# Initialize models
def init_models(N):
    G = [Generator(img_size=img_size).to(device) for _ in range(N)]
    D = [Discriminator(img_size=img_size).to(device) for _ in range(N)]
    return G, D

# Define dimensions and number of models
latent_dim = 100      # Latent space dimension for the generator input
N = 5                 # Number of local generator-discriminator pairs
I = 100               # Number of training iterations
ID = 1                # Number of discriminator updates per generator update

# Initialize models
G, D = init_models(N)
G0 = Generator(img_size=img_size).to(device)
D0 = Discriminator(img_size=img_size).to(device)

# Optimizers for local and global models
optimizers_G = [optim.Adam(g.parameters(), lr=0.0002, betas=(0.5, 0.999)) for g in G]
optimizers_D = [optim.Adam(d.parameters(), lr=0.0002, betas=(0.5, 0.999)) for d in D]
optimizer_G0 = optim.Adam(G0.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D0 = optim.Adam(D0.parameters(), lr=0.0002, betas=(0.5, 0.999))

# Lists to store losses
losses_G0 = []
losses_D0 = []
losses_Gn = []
losses_Dn = []

epoch_losses_D0 = []
epoch_losses_G0 = []

os.makedirs(f"{path_to_gdrive}/out_frogs224", exist_ok=True)
os.makedirs(f"{path_to_gdrive}/weights", exist_ok=True)

"""## Training loop"""

"""Training Loop"""
# Training Loop
for epoch in range(I):  # Total number of epochs
    epoch_loss_D0 = 0  # Accumulator for D0 losses in this epoch
    epoch_loss_G0 = 0  # Accumulator for G0 losses in this epoch
    num_batches = 0    # Counter for the number of batches

    for real_data, _ in data_loader:  # Load real images
        num_batches += 1
        real_data = real_data.to(device)
        batch_size = real_data.size(0)
        noise = torch.randn(batch_size, latent_dim, device=device)

        # Step 1: Train Local (Gn, Dn) Pairs
        for n in range(N):
            for _ in range(ID):  # Update Dn ID times
                D[n].zero_grad()
                fake_data = G[n](noise).detach()  # Generate fake data
                real_loss = nn.BCELoss()(D[n](real_data), torch.ones(batch_size, 1, device=device))
                fake_loss = nn.BCELoss()(D[n](fake_data), torch.zeros(batch_size, 1, device=device))
                loss_D = real_loss + fake_loss
                loss_D.backward()
                optimizers_D[n].step()

            # Update Gn
            G[n].zero_grad()
            fake_data = G[n](noise)
            loss_G = nn.BCELoss()(D[n](fake_data), torch.ones(batch_size, 1, device=device))
            loss_G.backward()
            optimizers_G[n].step()

            # Store local losses
            losses_Dn.append(loss_D.item())
            losses_Gn.append(loss_G.item())

        # Step 2: Create Messenger Discriminators D_msg
        D_msg = [copy.deepcopy(D[n]) for n in range(N)]

        # Step 3: Train G0 against Messenger Discriminators
        G0.zero_grad()
        loss_G0 = 0
        for D_msg_n in D_msg:
            fake_data = G0(noise)
            loss_G0 += nn.BCELoss()(D_msg_n(fake_data), torch.ones(batch_size, 1, device=device))
        loss_G0.backward()
        optimizer_G0.step()
        losses_G0.append(loss_G0.item())
        epoch_loss_G0 += loss_G0.item() # Add G0 loss to accumulator

        # Step 4: Train Messenger Discriminators (D_msg) against G0
        for D_msg_n in D_msg:
            D_msg_n.zero_grad()
            fake_data = G0(noise).detach()
            real_loss_msg = nn.BCELoss()(D_msg_n(real_data), torch.ones(batch_size, 1, device=device))
            fake_loss_msg = nn.BCELoss()(D_msg_n(fake_data), torch.zeros(batch_size, 1, device=device))
            loss_D_msg = real_loss_msg + fake_loss_msg
            loss_D_msg.backward()

        # Step 5: Train D0 against all local generators
        D0.zero_grad()
        loss_D0 = 0
        for n in range(N):
            fake_data = G[n](noise).detach()
            loss_D0 += nn.BCELoss()(D0(fake_data), torch.zeros(batch_size, 1, device=device))
        loss_D0 += nn.BCELoss()(D0(real_data), torch.ones(batch_size, 1, device=device))
        loss_D0.backward()
        optimizer_D0.step()
        losses_D0.append(loss_D0.item())
        epoch_loss_D0 += loss_D0.item() # Add D0 loss to accumulator

    print(f"Epoch {epoch}, G0 Loss: {np.mean(losses_G0)}")

    # Calculate average loss for the epoch
    avg_loss_D0 = epoch_loss_D0 / num_batches
    avg_loss_G0 = epoch_loss_G0 / num_batches

    # Append epoch losses
    epoch_losses_D0.append(avg_loss_D0)
    epoch_losses_G0.append(avg_loss_G0)

    # Save model weights every 10 epochs
    if (epoch + 1) % 10 == 0:
        torch.save(G0.state_dict(), f"{path_to_gdrive}/weights/epoch_{epoch + 1}_G0.pth")
        torch.save(D0.state_dict(), f"{path_to_gdrive}/weights/epoch_{epoch + 1}_D0.pth")
        for n in range(N):
            torch.save(G[n].state_dict(), f"{path_to_gdrive}/weights/epoch_{epoch + 1}_G{n}.pth")
            torch.save(D[n].state_dict(), f"{path_to_gdrive}/weights/epoch_{epoch + 1}_D{n}.pth")


    # Save and display an image from G0
    with torch.no_grad():
        sample_noise = torch.randn(16, latent_dim, device=device)  # Generate noise
        generated_images = G0(sample_noise).view(-1, 3, img_size, img_size)  # Generate images
        grid = vutils.make_grid(generated_images, nrow=4, normalize=True)

        # Save image to disk
        image_filename = f"{path_to_gdrive}/out_frogs224/epoch_{epoch}_G0.png"
        vutils.save_image(grid, image_filename)

        # # Display image
        # plt.figure(figsize=(8, 8))
        # plt.axis("off")
        # plt.title(f"G0 Images at Epoch {epoch}")
        # plt.imshow(grid.permute(1, 2, 0).cpu())
        # plt.show()

"""## Results üê∏ üëΩ"""

# Plot Loss Curves
plt.figure(figsize=(12, 6))
plt.title("Loss During Training")
plt.plot(epoch_loss_G0, label="G0 Loss")
plt.plot(epoch_loss_D0, label="D0 Loss")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.legend()
# Save the plot to an image file (e.g., PNG)
plt.savefig(f"{path_to_gdrive}/loss_curve.png")  # You can change the file name and extension (e.g., .jpg, .pdf)

outputs = os.listdir(f"{path_to_gdrive}/out_frogs")
outputs.sort(key=lambda x: int(x.split("_")[1].split(".")[0]))
for output in outputs:
    epoch = int(output.split('_')[1].split('.')[0])
    if (epoch % 20) != 1:
        continue
    plt.figure(figsize=(4, 4))
    plt.axis("off")
    plt.title(f"G0 Images at Epoch {epoch}")
    plt.imshow(Image.open(os.path.join(f"{path_to_gdrive}/out_frogs", output)))
    plt.show()